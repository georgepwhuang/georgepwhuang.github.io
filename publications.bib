@misc{huang2023postvariational,
      title={Post-variational quantum neural networks}, 
      author={Po-Wei Huang and Patrick Rebentrost},
      year={2023},
      eprint={2307.10560},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@article{huang-2022-domain,
    title = "Domain Specific Augmentations as Low Cost Teachers for Large Students",
    author = "Huang, Po-Wei",
    journal = "Proceedings of the first Workshop on Information Extraction from Scientific Publications",
    month = nov,
    year = "2022",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wiesp-1.10",
    pages = "84--90",
    abstract = "Current neural network solutions in scientific document processing employ models pretrained on domain-specific corpi, which are usually limited in model size, as pretraining can be costly and limited by training resources. We introduce a framework that uses data augmentation from such domain-specific pretrained models to transfer domain specific knowledge to larger general pretrained models and improve performance on downstream tasks. Our method improves the performance of Named Entity Recognition in the astrophysical domain by more than 20{\%} compared to domain-specific pretrained models finetuned to the target dataset.",
}

@inproceedings{huang-etal-2022-lightweight,
    title = "Lightweight Contextual Logical Structure Recovery",
    author = "Huang, Po-Wei  and
      Ramesh Kashyap, Abhinav  and
      Qin, Yanxia  and
      Yang, Yajing  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the Third Workshop on Scholarly Document Processing",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.sdp-1.5",
    pages = "37--48",
    abstract = "Logical structure recovery in scientific articles associates text with a semantic section of the article. Although previous work has disregarded the surrounding context of a line, we model this important information by employing line-level attention on top of a transformer-based scientific document processing pipeline. With the addition of loss function engineering and data augmentation techniques with semi-supervised learning, our method improves classification performance by 10{\%} compared to a recent state-of-the-art model. Our parsimonious, text-only method achieves a performance comparable to that of other works that use rich document features such as font and spatial position, using less data without sacrificing performance, resulting in a lightweight training pipeline.",
}